{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\"><td><a target=\"_blank\" href=\"https://colab.research.google.com/github/superannotateai/model-deployment-tutorials/blob/main/OAK/SuperAnnotate_OAK_Deeplabv3%2B_Deployment.ipynb\"><img src=\"https://user-images.githubusercontent.com/25985824/104791629-6e618700-5769-11eb-857f-d176b37d2496.png\" height=\"32\" width=\"32\"> Try in Google Colab</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upVdTuM6i-FI"
   },
   "source": [
    "# 1. Install Prerequisites \n",
    "(Please click on **RESTART RUNTIME** button when it appears in the output of this code block)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BY43J8a1lG8b",
    "outputId": "ef2f2072-3df4-4af6-f6dc-cb7a48fda4e9"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install -y pciutils cpio\n",
    "!sudo apt autoremove\n",
    "!wget http://registrationcenter-download.intel.com/akdlm/irc_nas/16345/l_openvino_toolkit_p_2020.1.023.tgz\n",
    "path = \"l_openvino_toolkit_p_2020.1.023.tgz\"\n",
    "!tar xf \"{path}\"\n",
    "%cd l_openvino_toolkit_p_2020.1.023/\n",
    "!./install_openvino_dependencies.sh && \\\n",
    "    sed -i 's/decline/accept/g' silent.cfg && \\\n",
    "    ./install.sh --silent silent.cfg\n",
    "!bash /content/l_openvino_toolkit_p_2020.1.023/install_openvino_dependencies.sh\n",
    "!bash /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites.sh\n",
    "\n",
    "%cd /content/\n",
    "\n",
    "!pip install tensorflow-gpu==1.15.3\n",
    "!pip install tf_slim==1.0.0\n",
    "\n",
    "!git clone https://github.com/nolanliou/mobile-deeplab-v3-plus\n",
    "!pip install superannotate\n",
    "!pip install google-resumable-media==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NDukI5gjVoY"
   },
   "source": [
    "# 2. Setup your SuperAnnotate token, project names of trianing images and desired output model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhvXpjlY-G3e"
   },
   "outputs": [],
   "source": [
    "TOKEN = \"For Pro Users, the Token-ID can be generated in the teams section\"\n",
    "PROJECT_NAME = \"City\"\n",
    "OUTPUT_MODEL = \"sa-tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X6NHoQdjbTf"
   },
   "source": [
    "# 3. Download and preprocess the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPKE00by-IGE",
    "outputId": "76b15f0f-3b97-4f6a-9e43-0b07ce31527f"
   },
   "outputs": [],
   "source": [
    "import superannotate as sa\n",
    "import json\n",
    "import os \n",
    "from shutil import copy, make_archive\n",
    "import re\n",
    "\n",
    "proj_folder = dict(zip(PROJECT_NAME, [\"proj_\" + str(i) for i in range(len(PROJECT_NAME))]))\n",
    "token_json = {\"token\": TOKEN}\n",
    "with open('sa_config.json', 'w') as f:\n",
    "  json.dump(token_json, f)\n",
    "\n",
    "sa.init('sa_config.json')\n",
    "\n",
    "dataset_base_dir = '/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg'\n",
    "if not os.path.exists(dataset_base_dir):\n",
    "  os.mkdir(dataset_base_dir)\n",
    "\n",
    "export = sa.prepare_export(PROJECT_NAME, annotation_statuses=[\"Completed\"], include_fuse=True)\n",
    "sa.download_export(PROJECT_NAME, export, dataset_base_dir)\n",
    "\n",
    "if not os.path.exists(dataset_base_dir + '/VOC2012'):\n",
    "  os.makedirs(dataset_base_dir + '/VOC2012/JPEGImages')\n",
    "  os.makedirs(dataset_base_dir + '/VOC2012/SegmentationClass')\n",
    "  os.makedirs(dataset_base_dir + '/VOC2012/ImageSets/Segmentation')\n",
    "  os.makedirs(dataset_base_dir + '/tfrecord')\n",
    "\n",
    "%cd /content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/\n",
    "!rm -f *.json\n",
    "!rm -f *___save.png\n",
    "\n",
    "folder_content = os.listdir('.')\n",
    "for entry in folder_content:\n",
    "  if len(entry.split('.'))==1:\n",
    "    continue\n",
    "  if entry.endswith('___fuse.png'):\n",
    "    im_name, _ = os.path.splitext(entry[:-11])\n",
    "    copy(entry, 'VOC2012/SegmentationClass/' + im_name + '.png')\n",
    "  else:\n",
    "    copy(entry, 'VOC2012/JPEGImages/' + entry)\n",
    "\n",
    "with open(dataset_base_dir + '/VOC2012/ImageSets/Segmentation/trainval.txt', 'w') as f:\n",
    "  image_names = os.listdir(dataset_base_dir + '/VOC2012/JPEGImages')\n",
    "  for image_name in image_names:\n",
    "    file_name, _ = os.path.splitext(image_name)\n",
    "    f.write(file_name + '\\n')\n",
    "  \n",
    "data_size = len(image_names)\n",
    "val_size = int(data_size * 0.1)\n",
    "train_size = data_size - val_size\n",
    "!shuf VOC2012/ImageSets/Segmentation/trainval.txt | tee >(head -n \"{val_size}\" > VOC2012/ImageSets/Segmentation/val.txt) | tail -n \"{train_size}\" > VOC2012/ImageSets/Segmentation/train.txt\n",
    "!python ../remove_gt_colormap.py --original_gt_folder=\"VOC2012/SegmentationClass\" --output_dir=\"VOC2012/SegmentationClassRaw\"\n",
    "\n",
    "%cd /content/mobile-deeplab-v3-plus/datasets/\n",
    "\n",
    "with open('build_voc2012_data.py') as f:\n",
    "  record_gen = f.read()\n",
    "\n",
    "record_gen = re.sub(r\"import pdb\\; pdb\\.set\\_trace\\(\\)\\n  \", \"\", record_gen)\n",
    "\n",
    "with open('build_voc2012_data.py', 'w') as f:\n",
    "  f.write(record_gen)\n",
    "\n",
    "!python build_voc2012_data.py \\\n",
    "  --image_folder=\"pascal_voc_seg/VOC2012/JPEGImages\" \\\n",
    "  --semantic_segmentation_folder=\"pascal_voc_seg/VOC2012/SegmentationClassRaw\" \\\n",
    "  --list_folder=\"pascal_voc_seg/VOC2012/ImageSets/Segmentation\" \\\n",
    "  --image_format=\"jpg\" \\\n",
    "  --output_dir=\"pascal_voc_seg/tfrecord/\"\n",
    "\n",
    "%cd /content/mobile-deeplab-v3-plus/\n",
    "objdata = 'segmentation_dataset.py'\n",
    "with open(objdata) as f:\n",
    "    s = f.read()\n",
    "\n",
    "with open('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/classes/classes.json', 'r') as f:\n",
    "  class_data = json.load(f)\n",
    "\n",
    "num_classes = len(class_data) + 1\n",
    "s = re.sub('num_classes=21','num_classes=' + str(num_classes),s)\n",
    "s = re.sub(\"'train': 1464\",\"'train': \" + str(train_size),s)\n",
    "s = re.sub(\"'trainval': 2913\",\"'trainval': \" + str(data_size),s)\n",
    "s = re.sub(\"'val': 1449\",\"'val': \" + str(val_size),s)\n",
    "\n",
    "with open(objdata, 'w') as f:\n",
    "  f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKcwpEHBjpj9"
   },
   "source": [
    "# 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZAIf5rboLnr",
    "outputId": "62d016fc-1a1e-4d50-8c67-1a0c5603e3ab"
   },
   "outputs": [],
   "source": [
    "%env PYTHONPATH=/content/mobile-deeplab-v3-plus:/env/python\n",
    "\n",
    "import tensorflow as tf\n",
    "from mobilenet_v2 import MobilenetV2\n",
    "import urllib\n",
    "\n",
    "%cd /content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/\n",
    "if not os.path.exists('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2'):\n",
    "  os.mkdir('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2')\n",
    "\n",
    "%cd mnv2 \n",
    "!wget https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz\n",
    "!tar xvf mobilenet_v2_1.0_224.tgz\n",
    "\n",
    "if not os.path.exists('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2_adapted'):\n",
    "  os.mkdir('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2_adapted')\n",
    "\n",
    "%cd /content/mobile-deeplab-v3-plus/\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# For simplicity we just decode jpeg inside tensorflow.\n",
    "# But one can provide any input obviously.\n",
    "file_input = tf.placeholder(tf.string, ())\n",
    "\n",
    "image = tf.image.decode_jpeg(tf.read_file(file_input))\n",
    "\n",
    "images = tf.expand_dims(image, 0)\n",
    "images = tf.cast(images, tf.float32) / 128. - 1\n",
    "images.set_shape((None, None, None, 3))\n",
    "images = tf.image.resize_images(images, (224, 224))\n",
    "mobilenet_model = MobilenetV2()\n",
    "\n",
    "logits, endpoints = mobilenet_model.forward(images, is_training=False)\n",
    "\n",
    "exclude = ['global_step']\n",
    "variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n",
    "variables_map = {}\n",
    "for v in variables_to_restore:\n",
    "  old_name = v.name.split(':')[0]\n",
    "  old_name = old_name.replace('conv2d/kernel', 'weights')\n",
    "  old_name = old_name.replace('conv2d/bias', 'biases')\n",
    "  variables_map[old_name] = v\n",
    "tf.train.init_from_checkpoint('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2/mobilenet_v2_1.0_224.ckpt', variables_map)\n",
    "filename, _ = urllib.request.urlretrieve('https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  x = endpoints['Predictions'].eval(feed_dict={file_input: filename})\n",
    "  saver.save(sess, '/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2_adapted/mobilenet-v2-224.ckpt')\n",
    "\n",
    "if not os.path.exists(\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/checkpoint\"):\n",
    "  os.mkdir(\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/checkpoint\")\n",
    "\n",
    "train_iters = 15000\n",
    "%cd /content/mobile-deeplab-v3-plus/\n",
    "!python run.py --dataset_dir=\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/tfrecord\"\\\n",
    "  --dataset_name=\"pascal_voc2012\"\\\n",
    "  --logdir=\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/checkpoint\"\\\n",
    "  --model_type=\"deeplab-v3-plus\"\\\n",
    "  --train_subset=\"train\"\\\n",
    "  --base_learning_rate=0.001\\\n",
    "  --num_clones=1\\\n",
    "  --model_input_size=256\\\n",
    "  --model_input_size=256\\\n",
    "  --training_number_of_steps=\"{train_iters}\"\\\n",
    "  --pretrained_backbone_model_dir=\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2_adapted\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK96y2i6jrp9"
   },
   "source": [
    "# 5. Generate and download the files needed for the OAK-D device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "id": "HyYgxtY0LC9d",
    "outputId": "5c9cfb3b-e708-4081-cf0d-06dc4768e601"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from google.colab import files\n",
    "\n",
    "if not os.path.exists('/content/export'):\n",
    "  os.mkdir('/content/export')\n",
    "\n",
    "!python run.py --dataset_dir=\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/tfrecord\"\\\n",
    "  --dataset_name=\"pascal_voc2012\"\\\n",
    "  --logdir=\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/checkpoint\"\\\n",
    "  --model_type=\"deeplab-v3-plus\"\\\n",
    "  --model_input_size=256\\\n",
    "  --model_input_size=256\\\n",
    "  --mode=export\\\n",
    "  --export_dir=\"/content/export\"\n",
    "\n",
    "!python freeze.py --model_dir=\"/content/export\"\\\n",
    "  --output_node_names=Output \\\n",
    "  --output_dir=\"/content\"\n",
    "\n",
    "%cd /content/\n",
    "!source /opt/intel/openvino/bin/setupvars.sh && \\\n",
    "    python /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py \\\n",
    "--input_model /content/frozen_model.pb \\\n",
    "--input_shape [1,256,256,3] \\\n",
    "--mean_values=[127.5,127.5,127.5] \\\n",
    "--scale_values=[255] \\\n",
    "--reverse_input_channel \\\n",
    "--data_type FP16\n",
    "\n",
    "os.mkdir(\"/content/\" + OUTPUT_MODEL)\n",
    "blob_dir = \"/content/\" + OUTPUT_MODEL\n",
    "\n",
    "binfile = \"/content/frozen_model.bin\"\n",
    "xmlfile = \"/content/frozen_model.xml\"\n",
    "\n",
    "url = \"http://69.164.214.171:8083/compile\"\n",
    "params = {\n",
    "    'version': '2020.1'\n",
    "}\n",
    "payload = {\n",
    "    'compile_type': 'myriad', \n",
    "    'compiler_params': '-ip U8 -VPU_MYRIAD_PLATFORM VPU_MYRIAD_2480 -VPU_NUMBER_OF_SHAVES 14 -VPU_NUMBER_OF_CMX_SLICES 14'\n",
    "}\n",
    "\n",
    "model_files = {\n",
    "    'definition': open(xmlfile, 'rb'),\n",
    "    'weights': open(binfile, 'rb')\n",
    "}\n",
    "response = requests.post(url, data=payload, files=model_files, params=params)\n",
    "blobname = 'deeplabv3p_person.blob.sh14cmx14NCE1'\n",
    "with open(blob_dir + '/' + blobname, 'wb') as f:\n",
    "  f.write(response.content)\n",
    "\n",
    "!wget https://raw.githubusercontent.com/superannotateai/model-deployment-tutorials/main/OAK/deeplabv3p_person.py -O \"/content/{OUTPUT_MODEL}/deeplabv3p_person.py\"\n",
    "\n",
    "make_archive(OUTPUT_MODEL, 'zip', OUTPUT_MODEL)\n",
    "files.download(OUTPUT_MODEL + '.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUum5lRhS60d"
   },
   "source": [
    "# 6. Copy and paste the generated model in 'depthai' directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4pbRWP3S-Cz"
   },
   "source": [
    "Please follow these steps on your local machine to get the model running\n",
    "\n",
    "**1.** Extract the downloaded \\$OUTPUT_MODEL.zip archive.\n",
    "\n",
    "**2.** Move 'deeplabv3p_person.blob.sh14cmx14NCE1' file to \\$DEPTHAI_ROOT_DIR/resources/nn/deeplabv3p_person folder.\n",
    "\n",
    "**3.** Rename the default 'deeplabv3p_person.py' in \\$DEPTHAI_ROOT_DIR/depthai_helpers/ folder.\n",
    "\n",
    "**4.** Move 'deeplabv3p_person.py' file from the extracted archive to \\$DEPTHAI_ROOT_DIR/depthai_helpers/ folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fbqBDdyU1Da"
   },
   "source": [
    "# 7. Test the custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSMG-HaPU_Dh"
   },
   "source": [
    "Afterwards you can run the new model with the following command from the $DEPTHAI_ROOT_DIR directory:\n",
    "\n",
    "```\n",
    "python3 depthai_demo.py -dd -cnn deeplabv3p_person\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qUum5lRhS60d",
    "0fbqBDdyU1Da"
   ],
   "name": "SuperAnnotate/OAK: Deeplabv3+ Deployment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
