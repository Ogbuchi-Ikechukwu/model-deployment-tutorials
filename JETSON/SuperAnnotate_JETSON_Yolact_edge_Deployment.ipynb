{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\"><td><a target=\"_blank\" href=\"https://colab.research.google.com/github/superannotateai/model-deployment-tutorials/blob/main/JETSON/SuperAnnotate_JETSON_Yolact_edge_Deployment.ipynb\"><img src=\"https://user-images.githubusercontent.com/25985824/104791629-6e618700-5769-11eb-857f-d176b37d2496.png\" height=\"32\" width=\"32\"> Try in Google Colab</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsI70dYsC-8W"
   },
   "source": [
    "# 0. Preparation Steps\n",
    "Before starting the pipeline you need to download tgz archives of TensorRT and CUDNN and place them in your google drive. (This notebook is tested with TensorRT 7.2.3.4 and CUDNN-11.2 versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9TtVkpKDFd5"
   },
   "source": [
    "# 1. Install Prerequisites \n",
    "(Please click on **RESTART RUNTIME** button when it appears in the output of this code block)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x_e_dd68ZKO"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsbrZqfRieEw"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.1.tar.gz /content/\n",
    "!cp /content/drive/MyDrive/cudnn-11.2-linux-x64-v8.1.1.33.tgz /content/\n",
    "\n",
    "!tar -xzvf cudnn-11.2-linux-x64-v8.1.1.33.tgz\n",
    "\n",
    "!sudo cp cuda/include/cudnn*.h /usr/local/cuda/include \n",
    "!sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda/lib64 \n",
    "!sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*\n",
    "\n",
    "!tar xzvf TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.1.tar.gz\n",
    "os.environ['LD_LIBRARY_PATH'] += ':/content/TensorRT-7.2.3.4/lib/'\n",
    "\n",
    "%cd /content/TensorRT-7.2.3.4/python/\n",
    "!sudo pip3 install tensorrt-7.2.3.4-cp37-none-linux_x86_64.whl\n",
    "\n",
    "%cd /content/TensorRT-7.2.3.4/uff/\n",
    "!sudo pip3 install uff-0.6.9-py2.py3-none-any.whl\n",
    "\n",
    "%cd /content/TensorRT-7.2.3.4/graphsurgeon/\n",
    "!sudo pip3 install graphsurgeon-0.4.5-py2.py3-none-any.whl\n",
    "%cd /content/TensorRT-7.2.3.4/onnx_graphsurgeon\n",
    "!sudo pip3 install onnx_graphsurgeon-0.2.6-py2.py3-none-any.whl\n",
    "\n",
    "%cd /content/\n",
    "!git clone https://github.com/haotian-liu/yolact_edge.git\n",
    "!git clone https://github.com/NVIDIA-AI-IOT/torch2trt\n",
    "\n",
    "!pip install gitpython\n",
    "!pip install superannotate\n",
    "!pip install google-resumable-media==0.5.0\n",
    "!pip install onnx-simplifier\n",
    "\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran\n",
    "!sudo apt-get install python3-pip\n",
    "!pip3 install -U pip testresources setuptools==49.6.0\n",
    "!pip3 install future==0.18.2 mock==3.0.5 h5py==2.10.0 keras_preprocessing==1.1.1 keras_applications==1.0.8 gast==0.2.2 futures protobuf pybind11\n",
    "!pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v43 'tensorflow<2'\n",
    "!pip3 install pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXcVy1VnDOGf"
   },
   "source": [
    "# 2. Setup your SuperAnnotate token, project names of trianing images and desired output model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMS0XpsAkfUC"
   },
   "outputs": [],
   "source": [
    "TOKEN = \"For Pro Users, the Token-ID can be generated in the teams section\"\n",
    "PROJECT_NAME = \"City\"\n",
    "OUTPUT_MODEL = \"sa-tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzHPqlGxERpl"
   },
   "source": [
    "# 3. Download and preprocess the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f55IUCylk5Cr"
   },
   "outputs": [],
   "source": [
    "import superannotate as sa\n",
    "import json\n",
    "import os \n",
    "from shutil import copy, make_archive\n",
    "import re\n",
    "\n",
    "os.environ['CUDA_INSTALL_DIR']= \"/usr/local/cuda/\"\n",
    "os.environ['CUDNN_INSTALL_DIR']= \"/usr/local/cuda/\"\n",
    "os.environ['LD_LIBRARY_PATH']+= \":/usr/lib64-nvidia/:/content/TensorRT-7.2.3.4/include/:/content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib/:/content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/bin/\"\n",
    "os.environ['PATH']+= \":/content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib:/content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/bin/\"\n",
    "!sudo rm /content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib/libnvinfer.so.7\n",
    "!sudo ln -s /content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib/libnvinfer.so.7.2.3 /content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib/libnvinfer.so.7\n",
    "!sudo cp -r /content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib/*.* /usr/lib/ \n",
    "!sudo cp -r /content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/bin/*.* /usr/bin/ \n",
    "!sudo cp -r /content/TensorRT-7.2.3.4/include/* /usr/local/cuda/targets/x86_64-linux/include/\n",
    "!sudo cp -r /content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib/* /usr/local/cuda/targets/x86_64-linux/lib/\n",
    "\n",
    "proj_folder = dict(zip(PROJECT_NAME, [\"proj_\" + str(i) for i in range(len(PROJECT_NAME))]))\n",
    "token_json = {\"token\": TOKEN}\n",
    "with open('sa_config.json', 'w') as f:\n",
    "  json.dump(token_json, f)\n",
    "\n",
    "sa.init('sa_config.json')\n",
    "\n",
    "dataset_base_dir = '/content/data'\n",
    "if not os.path.exists(dataset_base_dir):\n",
    "  os.mkdir(dataset_base_dir)\n",
    "\n",
    "export = sa.prepare_export(PROJECT_NAME, annotation_statuses=[\"Completed\"], include_fuse=True)\n",
    "sa.download_export(PROJECT_NAME, export, dataset_base_dir)\n",
    "sa.export_annotation(\"/content/data\", \"/content/data_coco\", \"COCO\", PROJECT_NAME, project_type=\"Pixel\", task=\"instance_segmentation\")\n",
    "\n",
    "with open('/content/data/classes/classes.json', 'r') as f:\n",
    "  classes_data = json.load(f)\n",
    "\n",
    "class_names = []\n",
    "label_map = {}\n",
    "for i, class_data in enumerate(classes_data):\n",
    "  class_names.append(class_data[\"name\"])\n",
    "  label_map[class_data[\"id\"]] = i + 1\n",
    "\n",
    "dataset_name = \"Custom train\"\n",
    "img_paths = '/content/data_coco/image_set/'\n",
    "annot_path = '/content/data_coco/' + PROJECT_NAME + '.json'\n",
    "dataset_dict = {\n",
    "  'name': dataset_name,\n",
    "  'train_images': img_paths,\n",
    "  'train_info':   annot_path,\n",
    "  'valid_images': img_paths,\n",
    "  'valid_info':   annot_path,\n",
    "  'has_gt': True,\n",
    "  'class_names': tuple(class_names),\n",
    "  'label_map': label_map,\n",
    "}\n",
    "\n",
    "yolact_conf_dict = {\n",
    "  'dataset': 'my_custom_dataset',\n",
    "  'num_classes': len(class_names) + 1,\n",
    "  'name': 'yolact_edge_mobilenetv2_custom',\n",
    "  'backbone': 'mobilenetv2_backbone'\n",
    "}\n",
    "dataset_line = \"my_custom_dataset = dataset_base.copy(\" + str(dataset_dict) + \")\"\n",
    "\n",
    "yolact_conf_line = \"yolact_edge_config_custom = yolact_edge_config.copy(\" + str(yolact_conf_dict) + \")\"\n",
    "\n",
    "with open('/content/yolact_edge/data/config.py', 'a') as f:\n",
    "  f.write('\\n')\n",
    "  f.write(dataset_line)\n",
    "  f.write('\\n')\n",
    "  f.write(yolact_conf_line)\n",
    "\n",
    "label_map = \"'label_map': \" + str(label_map)\n",
    "with open('/content/yolact_edge/data/config.py') as f:\n",
    "    s = f.read()\n",
    "\n",
    "s = re.sub(\"'label_map': None\", label_map, s)\n",
    "s = re.sub(\"'label_map': COCO_LABEL_MAP\", label_map, s)\n",
    "s = re.sub(\"'dataset': 'my_custom_dataset'\",\"'dataset': my_custom_dataset\", s)\n",
    "s = re.sub(\"'backbone': 'mobilenetv2_backbone'\",\"'backbone': mobilenetv2_backbone\", s)\n",
    "\n",
    "with open('/content/yolact_edge/data/config.py', 'w') as f:\n",
    "  f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMzzo0DWEZD1"
   },
   "source": [
    "# 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufjbwZHp42CS"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/yolact_edge/weights\n",
    "%cd /content/yolact_edge/weights\n",
    "!wget 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth'\n",
    "\n",
    "%cd /content/yolact_edge/\n",
    "\n",
    "!python train.py --config=yolact_edge_config_custom --dataset=my_custom_dataset --num_gpus 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nP2yIw7xEcAL"
   },
   "source": [
    "# 5. Generate and download the files needed for the Jetson device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIrTxNOlHfjC"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "%cd /content/torch2trt\n",
    "!sudo python setup.py install --plugins\n",
    "\n",
    "%cd /content/yolact_edge/\n",
    "!wget https://c402277.ssl.cf1.rackcdn.com/photos/18315/images/hero_small/Medium_WW230176.jpg\n",
    "!python eval.py --config=yolact_edge_config_custom\\\n",
    " --trained_model=\"latest\"\\\n",
    " --image=\"Medium_WW230176.jpg\"\\\n",
    " --use_fp16_tensorrt\\\n",
    " --use_tensorrt_safe_mode\n",
    " \n",
    "!zip -r \"{OUTPUT_MODEL}.zip\" /content/yolact_edge/weights/*.trt\n",
    "files.download(OUTPUT_MODEL + '.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_eYATaMQz1R"
   },
   "source": [
    "# 6. Test the custom model\n",
    "\n",
    "Afterwards you can load the files in the archive with tensorrt on jetson devices and integrate it to your pipelines.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SuperAnnotate/Jetson: Yolact_edge Deployment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
